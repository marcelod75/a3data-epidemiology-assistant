A3Data Epidemiology Assistant

Assistente RAG (Retrieval-Augmented Generation) para responder perguntas sobre os RelatÃ³rios EpidemiolÃ³gicos 2021 (UE/EEA).
Pipeline: ingestÃ£o de PDFs â†’ ChromaDB (embeddings) â†’ FastAPI (/ask) â†’ LLM OpenAI (opcional).

âœ¨ O que ele faz

Faz busca semÃ¢ntica em chunks dos relatÃ³rios (ChromaDB).

Gera respostas baseadas nos trechos recuperados (RAG).

Cita fontes (arquivo + chunk) no final da resposta.

Possui Swagger UI em http://127.0.0.1:8000/docs.

ğŸ§± PrÃ©-requisitos

Windows + PowerShell

Python 3.11+

(Opcional) Chave da OpenAI para respostas com LLM
Sem chave, a API retorna â€œSem LLM configuradoâ€ + trechos relevantes (Ãºtil para demo).

ğŸš€ Setup rÃ¡pido (5 min)
# 1) clonar e entrar
git clone https://github.com/marcelod75/a3data-epidemiology-assistant.git
cd a3data-epidemiology-assistant

# 2) criar venv e instalar deps
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

Criar .env

Crie um arquivo .env na raiz (ou duplique o .env.example):

OPENAI_API_KEY=sua_chave_aqui   # opcional (sem isso, vem resposta "Sem LLM configurado")
OPENAI_MODEL=gpt-4o-mini
PERSIST_DIR=artifacts/chroma
COLLECTION_NAME=eu_epi_2021


Importante: .env nÃ£o deve ir para o git (jÃ¡ estÃ¡ ignorado no .gitignore).

ğŸ“¥ IngestÃ£o (se for indexar PDFs)

Coloque os PDFs em data/.

Gere o arquivo documents.jsonl (chunks) e depois embeddings no Chroma:

# Gera o JSONL com chunks (ajuste tamanhos se quiser)
python src/ingestion.py `
  --data_dir data `
  --out_path artifacts/documents.jsonl `
  --chunk_size 1200 `
  --chunk_overlap 200

# Cria embeddings no ChromaDB
python src/embedding_pipeline.py `
  --jsonl_path artifacts/documents.jsonl `
  --persist_dir artifacts/chroma


Se vocÃª jÃ¡ tem artifacts/chroma pronto, nÃ£o precisa rodar a ingestÃ£o.

â–¶ï¸ Subir a API
uvicorn src.api:app --reload --port 8000
# Abra: http://127.0.0.1:8000/docs

ğŸ§ª Testes (prontos para colar)
1) Swagger UI (POST /ask)

Exemplo 1 â€” panorama TB:

{
  "question": "Qual foi o panorama da tuberculose em 2021 na UE/EEA? Resuma dados e tendÃªncias.",
  "k": 5
}


Exemplo 2 â€” doenÃ§as respiratÃ³rias monitoradas:

{
  "question": "Quais doenÃ§as respiratÃ³rias foram monitoradas em 2021?",
  "k": 5
}


Exemplo 3 â€” populaÃ§Ãµes vulnerÃ¡veis:

{
  "question": "Quais populaÃ§Ãµes foram destacadas como mais vulnerÃ¡veis Ã s doenÃ§as respiratÃ³rias em 2021 e por quÃª?",
  "k": 6
}


Exemplo 4 â€” recomendaÃ§Ãµes de vigilÃ¢ncia:

{
  "question": "Quais foram as recomendaÃ§Ãµes de vigilÃ¢ncia para Zika em 2021?",
  "k": 5
}

2) PowerShell (sem Postman)
$payload = @{ question = "Qual foi o panorama da tuberculose em 2021 na UE/EEA?"; k = 5 } | ConvertTo-Json
Invoke-RestMethod -Method Post `
  -Uri http://127.0.0.1:8000/ask `
  -ContentType 'application/json; charset=utf-8' `
  -Body ([System.Text.Encoding]::UTF8.GetBytes($payload))

ğŸ“Œ Expectativa de resposta

Com chave OpenAI vÃ¡lida, a resposta vem sintetizada + â€œFontes: arquivo#chunkâ€¦â€

Sem chave, volta: â€œSem LLM configurado. Trechos relevantes: â€¦â€ + fontes
(Ãºtil para provar a recuperaÃ§Ã£o de contexto mesmo sem LLM).

ğŸ—‚ï¸ Estrutura do projeto
a3data-epidemiology-assistant/
â”œâ”€ data/                      # PDFs (local; nÃ£o versionar)
â”œâ”€ artifacts/
â”‚  â”œâ”€ chroma/                 # banco vetorial (gerado)
â”‚  â””â”€ documents.jsonl         # saÃ­da de ingestÃ£o (gerado)
â”œâ”€ src/
â”‚  â”œâ”€ api.py                  # FastAPI (endpoint /ask)
â”‚  â”œâ”€ rag_agent.py            # retrieve + generate (OpenAI opcional)
â”‚  â”œâ”€ ingestion.py            # lÃª PDFs, limpa, chunca
â”‚  â””â”€ embedding_pipeline.py   # grava embeddings no Chroma
â”œâ”€ .env                       # suas variÃ¡veis (NÃƒO commit)
â”œâ”€ .env.example               # modelo seguro
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â””â”€ README.md

ğŸ§° Troubleshooting

PowerShell bloqueia Activate.ps1
Abra um novo terminal e rode:

Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
.\.venv\Scripts\Activate.ps1


Porta 8000 ocupada

netstat -aon | findstr :8000
taskkill /PID <PID> /F


ModuleNotFoundError: No module named 'rag_agent'
Execute a partir da raiz do projeto:

uvicorn src.api:app --reload --port 8000


Ou use esse launch.json no VS Code:

{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Run Uvicorn API",
      "type": "debugpy",
      "request": "launch",
      "module": "uvicorn",
      "args": ["src.api:app", "--reload", "--port", "8000"],
      "justMyCode": true
    }
  ]
}


Chroma com erro em filtros
Use apenas query_texts + n_results (operadores permitidos: $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin).

ğŸ“£ Roteiro de demo (sugestÃµes para o cliente)

Mostrar http://127.0.0.1:8000/docs e o endpoint /ask.

Enviar 2â€“3 perguntas de negÃ³cio (exemplos acima).

Mostrar as Fontes citadas (arquivo + chunk) â€” foco em auditoria e rastreabilidade.

Explicar que sem a chave OpenAI ainda Ã© possÃ­vel ver os trechos (validando o corpus).

Com a chave, a resposta vem sintetizada e pronta para executivo.

âœ… Status

âœ… IngestÃ£o/embeddings funcionando (ChromaDB).

âœ… API FastAPI operando (Swagger).

âœ… GeraÃ§Ã£o com OpenAI quando OPENAI_API_KEY estÃ¡ no .env.
